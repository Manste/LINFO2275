{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "datamining2b.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "tDdYk5MvfiqA"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "\n",
        "LIMIT = 10\n",
        "class Dataset:\n",
        "    def __init__(self, LIMIT=10, dir_path=\"drive/MyDrive/Datasets\"):\n",
        "        self.dataset = [[[] for _ in range(10)] for _ in range(10)]\n",
        "        self.data = []\n",
        "        self.labels = []\n",
        "        limit_the_search = [j for i in range(10) for k in range(10) for j in range(i*100+(k*10)+1, i*100+(k*10)+LIMIT+1)]\n",
        "        for i in limit_the_search:\n",
        "          filepath = \"{}/{}.txt\".format(dir_path, i)\n",
        "          try:\n",
        "              lines = [line.strip() for line in open(filepath, \"r\")]\n",
        "              class_id =  int(lines[1][len(lines[1]) - 1])\n",
        "              user_id = int(lines[2][len(lines[2]) - 1])\n",
        "              user_data = []\n",
        "              for i, line in enumerate(lines[5:]):\n",
        "                  if line:\n",
        "                      user_data.append(list(map(float, line.split(',')))[0:3])\n",
        "              self.dataset[user_id - 1][class_id - 1].append(user_data) \n",
        "              self.data.append(user_data)\n",
        "              self.labels.append(class_id)\n",
        "          except IOError as e:\n",
        "              print(\"Unable to read dataset file {}!\\n\".format(filepath))\n",
        "\n",
        "    def get_user(self, index):\n",
        "        \"\"\"Returns an array containing all the data saved for the user <<index>>\"\"\"\n",
        "        return len(self.data[index])\n",
        "\n",
        "dataset = Dataset()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import tslearn\n",
        "import sys\n",
        "!{sys.executable} -m pip install tslearn"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PspvZ9CRf25Q",
        "outputId": "f4937250-c683-4559-d8fc-c21a33ad886c"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: tslearn in /usr/local/lib/python3.7/dist-packages (0.5.2)\n",
            "Requirement already satisfied: Cython in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.29.28)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.1.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.0.2)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.7/dist-packages (from tslearn) (0.51.2)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.4.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from tslearn) (1.21.6)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (57.4.0)\n",
            "Requirement already satisfied: llvmlite<0.35,>=0.34.0.dev0 in /usr/local/lib/python3.7/dist-packages (from numba->tslearn) (0.34.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->tslearn) (3.1.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from tslearn.preprocessing import TimeSeriesScalerMinMax\n",
        "from tslearn.svm import TimeSeriesSVC\n",
        "from tslearn.utils import to_time_series_dataset\n",
        "from tslearn.neighbors import KNeighborsTimeSeriesClassifier\n",
        "\n",
        "np.random.seed(0)\n",
        "LIMIT= LIMIT*10\n",
        "\n",
        "x = np.array(dataset.data)\n",
        "labels = np.array(dataset.labels)\n",
        "\n",
        "accuracies = []\n",
        "for user_id in range(10):\n",
        "  # split the dataset\n",
        "  indexes = range(user_id*LIMIT, user_id*LIMIT+LIMIT)\n",
        "  #print(list(indexes))\n",
        "  train_set = to_time_series_dataset(np.delete(x, indexes))\n",
        "  train_labels = np.delete(labels, indexes)\n",
        "  test_labels = labels[indexes]\n",
        "  test_set = to_time_series_dataset(x[indexes])\n",
        "\n",
        "  x_train = TimeSeriesScalerMinMax().fit_transform(train_set)\n",
        "  x_test = TimeSeriesScalerMinMax().fit_transform(test_set)\n",
        "\n",
        "  model = TimeSeriesSVC(kernel=\"gak\", gamma=\"auto\", probability=True)\n",
        "  # Prediction\n",
        "  model.fit(x_train, train_labels)\n",
        "  accuracy = model.score(x_test, test_labels)\n",
        "  print(\"The accuracy pour l'utilisateur {}: {}\".format(user_id+1, accuracy))\n",
        "  accuracies.append(accuracy)"
      ],
      "metadata": {
        "id": "6qIkaUEKj6E_",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e951698-71c0-43aa-f025-5aade27248d7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:9: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
            "  if __name__ == '__main__':\n"
          ]
        }
      ]
    }
  ]
}